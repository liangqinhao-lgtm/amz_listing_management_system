"""Gigaå•†å“åº“å­˜åŒæ­¥æœåŠ¡"""
import os
import json
import logging
import time
from typing import List, Dict, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from sqlalchemy.orm import Session
from infrastructure.giga.api_client import GigaAPIClient, GigaAPIException
from infrastructure.db_pool import SessionLocal
from src.repositories.giga_product_inventory_repository import GigaProductInventoryRepository

logger = logging.getLogger(__name__)

class GigaInventorySyncService:
    """Gigaå•†å“åº“å­˜åŒæ­¥æœåŠ¡"""
    
    def __init__(
        self,
        db: Session,
        batch_size: int = 200,
        max_retries: int = 3,
        max_threads: int = 5,
        api_rate_limit: int = 9,
        wait_time: int = 10,
        save_api_response: bool = False
    ):
        self.db = db
        self.repository = GigaProductInventoryRepository(db)
        self.api_client = GigaAPIClient()
        
        # é…ç½®å‚æ•°
        self.batch_size = batch_size
        self.max_retries = max_retries
        self.max_threads = max_threads
        self.api_rate_limit = api_rate_limit
        self.wait_time = wait_time
        self.save_api_response = save_api_response
        
        # APIå“åº”ä¿å­˜ç›®å½•
        if self.save_api_response:
            self.response_dir = "api_responses/inventory"
            os.makedirs(self.response_dir, exist_ok=True)
    
    def _save_api_response(self, batch_idx: int, skus: List[str], response_data: Dict):
        """ä¿å­˜APIå“åº”åˆ°æ–‡ä»¶"""
        if not self.save_api_response:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(
                self.response_dir,
                f"inventory_response_{timestamp}_batch_{batch_idx}.json"
            )
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump({
                    "request": {
                        "timestamp": timestamp,
                        "batch_index": batch_idx,
                        "skus": skus,
                        "count": len(skus)
                    },
                    "response": response_data
                }, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"ä¿å­˜APIå“åº”å¤±è´¥: {e}")
    
    def fetch_batch_inventory(self, skus: List[str]) -> Dict:
        """
        è·å–æ‰¹æ¬¡å•†å“åº“å­˜ï¼ˆå¸¦é‡è¯•ï¼‰
        
        Args:
            skus: SKUåˆ—è¡¨
            
        Returns:
            APIå“åº”æ•°æ®
        """
        for attempt in range(self.max_retries):
            try:
                payload = {"skus": skus}
                response = self.api_client.execute(
                    "inventory_qty",
                    payload,
                    method="POST"
                )
                
                body = response.get('body', {})
                if not body:
                    raise ValueError("APIå“åº”ç»“æ„æ— æ•ˆ")
                
                return body
                
            except Exception as e:
                if attempt < self.max_retries - 1:
                    delay = 2 ** attempt
                    logger.warning(f"APIè°ƒç”¨å¤±è´¥ï¼Œ{delay}ç§’åé‡è¯• ({attempt+1}/{self.max_retries}): {e}")
                    time.sleep(delay)
                else:
                    logger.error(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    raise
    
    def process_batch(self, batch_idx: int, skus: List[str]) -> Tuple[int, int]:
        """
        å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
        
        Returns:
            (å¤„ç†æ•°é‡, æ›´æ–°æ•°é‡)
        """
        # ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯
        with SessionLocal() as thread_db:
            thread_repo = GigaProductInventoryRepository(thread_db)
            
            try:
                # 1. è°ƒç”¨API
                response = self.fetch_batch_inventory(skus)
                self._save_api_response(batch_idx, skus, response)
                
                # 2. è§£ææ•°æ®
                inventory_data = []
                for item in response.get("data", []):
                    try:
                        inventory_data.append(thread_repo.parse_inventory_item(item))
                    except Exception as e:
                        logger.error(f"è§£æåº“å­˜é¡¹å¤±è´¥: {e}")
                        continue
                
                # 3. æ‰¹é‡æ›´æ–°
                processed, upserted = thread_repo.bulk_upsert_inventory(inventory_data)
                
                logger.info(f"æ‰¹æ¬¡ {batch_idx}: å¤„ç† {processed} æ¡, æ›´æ–° {upserted} æ¡")
                return processed, upserted
                
            except Exception as e:
                logger.error(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
                return 0, 0
    
    def sync_all_inventory(self) -> Dict[str, int]:
        """
        åŒæ­¥å…¨é‡å•†å“åº“å­˜
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯: {'total_skus': x, 'processed': y, 'upserted': z}
        """
        logger.info("ğŸš€ å¼€å§‹åº“å­˜åŒæ­¥æµç¨‹...")
        start_time = time.time()
        
        stats = {
            'total_skus': 0,
            'batches': 0,
            'processed': 0,
            'upserted': 0,
            'success_batches': 0,
            'failed_batches': 0
        }
        
        try:
            # 1. è·å–æ‰€æœ‰SKU
            all_skus = self.repository.get_all_skus()
            stats['total_skus'] = len(all_skus)
            
            if not stats['total_skus']:
                logger.info("æ²¡æœ‰éœ€è¦æ›´æ–°çš„SKU")
                print("âœ… æ²¡æœ‰éœ€è¦æ›´æ–°çš„SKU")
                return stats
            
            # 2. åˆ†æ‰¹
            batches = [
                all_skus[i:i + self.batch_size]
                for i in range(0, stats['total_skus'], self.batch_size)
            ]
            stats['batches'] = len(batches)
            
            logger.info(f"å¾…åŒæ­¥SKUæ€»æ•°: {stats['total_skus']}, æ‰¹æ¬¡æ•°: {stats['batches']}")
            print(f"\nğŸ“Š å¾…åŒæ­¥SKUæ€»æ•°: {stats['total_skus']}")
            print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            print(f"ğŸ§µ çº¿ç¨‹æ•°: {self.max_threads}\n")
            
            # 3. ä½¿ç”¨çº¿ç¨‹æ± å¤„ç†
            with ThreadPoolExecutor(max_workers=min(self.max_threads, len(batches))) as executor:
                futures = {
                    executor.submit(self.process_batch, idx + 1, batch): idx
                    for idx, batch in enumerate(batches)
                }
                
                for future in as_completed(futures):
                    batch_idx = futures[future] + 1
                    
                    try:
                        processed, upserted = future.result()
                        stats['processed'] += processed
                        stats['upserted'] += upserted
                        stats['success_batches'] += 1
                        
                        print(f"âœ”ï¸ æ‰¹æ¬¡ {batch_idx}/{stats['batches']}: æ›´æ–° {upserted} æ¡")
                        
                    except Exception as e:
                        stats['failed_batches'] += 1
                        logger.error(f"æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                        print(f"âŒ æ‰¹æ¬¡ {batch_idx}/{stats['batches']}: å¤±è´¥")
                    
                    # è¿›åº¦æŠ¥å‘Š
                    progress = batch_idx / stats['batches'] * 100
                    logger.info(f"è¿›åº¦: {progress:.1f}% | æ‰¹æ¬¡: {batch_idx}/{stats['batches']}")
                    
                    # APIé™æµ
                    if batch_idx % self.api_rate_limit == 0:
                        time.sleep(self.wait_time)
        
        except Exception as e:
            logger.error(f"åŒæ­¥æµç¨‹å¼‚å¸¸: {e}")
        
        finally:
            # æœ€ç»ˆæŠ¥å‘Š
            elapsed = time.time() - start_time
            
            print("\n" + "="*60)
            print("âœ… åº“å­˜åŒæ­¥å®Œæˆï¼")
            print("="*60)
            print(f"SKUæ€»æ•°: {stats['total_skus']}")
            print(f"å¤„ç†æ‰¹æ¬¡: {stats['batches']}")
            print(f"æˆåŠŸæ‰¹æ¬¡: {stats['success_batches']}")
            print(f"å¤±è´¥æ‰¹æ¬¡: {stats['failed_batches']}")
            print(f"æ›´æ–°è®°å½•: {stats['upserted']}/{stats['processed']}")
            print(f"è€—æ—¶: {elapsed:.2f}ç§’ ({elapsed/60:.2f}åˆ†é’Ÿ)")
            print("="*60 + "\n")
            
            logger.info(f"åº“å­˜åŒæ­¥å®Œæˆ | SKUæ€»æ•°: {stats['total_skus']}")
            logger.info(f"æ›´æ–°è®°å½•: {stats['upserted']}/{stats['processed']}")
            logger.info(f"æ€»è€—æ—¶: {elapsed:.2f}ç§’")
        
        return stats
